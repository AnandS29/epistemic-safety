{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nashpy as nash\n",
    "import itertools\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining MDP - old game\n",
    "\n",
    "# state_names = [\"s1\", \"s2\", \"s3\", \"s4\"]\n",
    "# risk_levels = np.linspace(0,10,100)\n",
    "# states = []\n",
    "# for name in state_names:\n",
    "#     for risk in risk_levels:\n",
    "#         states.append((name, risk))\n",
    "\n",
    "# human_actions = [(\"c\",0), (\"c\",1), (\"d\",0), (\"d\",1)]\n",
    "# robot_actions = [(\"c\",0), (\"c\",1), (\"d\",0), (\"d\",1)]\n",
    "\n",
    "# def transition(state, uH, uR, dr):\n",
    "#     s = state[0]\n",
    "#     r = state[1] + dr\n",
    "#     if s == \"s1\":\n",
    "#         if 0 in uR:\n",
    "#             return (\"s2\",r)\n",
    "#         elif 1 in uR:\n",
    "#             return (\"s1\",r)\n",
    "#     elif s == \"s2\":\n",
    "#         if 0 in uH:\n",
    "#             return (\"s3\",r)\n",
    "#         elif 1 in uH:\n",
    "#             return (\"s1\",r)\n",
    "#     elif s == \"s3\":\n",
    "#         return (\"s3\",r)\n",
    "#     elif s == \"s4\":\n",
    "#         if 1 in uH and 1 in uR:\n",
    "#             return (\"s2\",r)\n",
    "#         else:\n",
    "#             return (\"s4\",r)\n",
    "#     else:\n",
    "#         print(\"Incorrect state\")\n",
    "        \n",
    "# def prisoners_dilemma(uH,uR):\n",
    "#     if uH == \"c\" and uR == \"c\":\n",
    "#         return (3,3)\n",
    "#     if uH == \"d\" and uR == \"d\":\n",
    "#         return (1,1)\n",
    "#     if uH == \"c\" and uR == \"d\":\n",
    "#         return (0,5)\n",
    "#     if uH == \"d\" and uR == \"c\":\n",
    "#         return (5,0)\n",
    "    \n",
    "# def pay_or_punish(uH,uR):\n",
    "#     if uR == \"c\":\n",
    "#         return (10,10)\n",
    "#     if uR == \"d\":\n",
    "#         return (-10,-10)\n",
    "    \n",
    "# def social(t):\n",
    "#         return (t[0],t[1],t[0]+t[1])\n",
    "        \n",
    "# def reward(state, uH, uR):\n",
    "#     s = state[0]\n",
    "#     if s == \"s1\":\n",
    "#         r = prisoners_dilemma(uH[0], uR[0])\n",
    "#     elif s == \"s2\":\n",
    "#         r = prisoners_dilemma(uH[0], uR[0])\n",
    "#     elif s == \"s3\":\n",
    "#         r = pay_or_punish(uH[0],uR[0])\n",
    "#     elif s == \"s4\":\n",
    "#         r = prisoners_dilemma(uH[0], uR[0])\n",
    "#     else:\n",
    "#         print(\"Incorrect state\")\n",
    "#         return None\n",
    "#     return social(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining MDP - manufacturing example\n",
    "\n",
    "state_names = [\"s1\"]\n",
    "risk_levels = np.linspace(0,100,100)\n",
    "states = []\n",
    "for name in state_names:\n",
    "    for risk in risk_levels:\n",
    "        states.append((name, risk))\n",
    "\n",
    "human_actions = [1,5,10]\n",
    "robot_actions = [\"S\",\"F\"]\n",
    "\n",
    "def project_risk(r):\n",
    "    return risk_levels[np.argmin([np.abs(r-l) for l in risk_levels])]\n",
    "\n",
    "def transition(state, uH, uR, dr):\n",
    "    s = state[0]\n",
    "    r = risk_levels[np.argmin([np.abs(state[1]+dr-l) for l in risk_levels])]\n",
    "    return (s,r)\n",
    "        \n",
    "def reward(state, uH, uR):\n",
    "    rR = (2 if uR==\"S\" else 0)*uH\n",
    "    rH = (10-uH)\n",
    "    social = rR + rH\n",
    "    return (social,social,social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "gamma = 1.0\n",
    "v_funs = {i:{s:None for s in states} for i in range(T+1)}\n",
    "v_funs_adv = {i:{s:None for s in states} for i in range(T+1)}\n",
    "v_funs_adv_p = {i:{s:None for s in states} for i in range(T+1)}\n",
    "uHs = {i:{s:None for s in states} for i in range(T)}\n",
    "uRs_coop = {i:{s:None for s in states} for i in range(T)}\n",
    "uRs_adv = {i:{s:None for s in states} for i in range(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value functions\n",
    "for s in states:\n",
    "    v_funs[T][s] = 0\n",
    "    v_funs_adv[T][s] = 0\n",
    "    v_funs_adv_p[T][s] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_combos(l,t):\n",
    "    l = [l[:] for _ in range(t)]\n",
    "    d = defaultdict(list)\n",
    "    for x in product(*l):\n",
    "        d[tuple(sorted(x))].append(x)\n",
    "\n",
    "    return [x[0] for x in d.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute baseline adversarial value function via brute force\n",
    "# for t in range(T):\n",
    "#     plans_h = get_combos(human_actions, T-t)\n",
    "#     plans_r = get_combos(robot_actions, T-t)\n",
    "#     for s in states:\n",
    "#         max_min_value = None\n",
    "#         for plan_h in plans_h:\n",
    "#             min_value = None\n",
    "#             for plan_r in plans_r:\n",
    "#                 running_reward = 0\n",
    "#                 s_curr = s\n",
    "#                 for i in range(T-t):\n",
    "#                     uH, uR = plan_h[i], plan_r[i]\n",
    "#                     running_reward += (gamma**i)*(reward(s_curr,uH,uR)[0])\n",
    "#                     s_curr = transition(s_curr,uH,uR,0)\n",
    "#                 if min_value is None or running_reward < min_value:\n",
    "#                     min_value = running_reward\n",
    "#             if max_min_value is None or max_min_value > min_value:\n",
    "#                 max_min_value = min_value\n",
    "#         v_funs_adv[t][s] = max_min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9, 8, 7, 6, 5, 4, 3, 2, 1, 0, "
     ]
    }
   ],
   "source": [
    "# Compute baseline adversarial value function\n",
    "t = T-1\n",
    "while t >= 0:\n",
    "    print(t, end=\", \")\n",
    "    for s in states:\n",
    "        A = np.zeros(shape=(len(human_actions), len(robot_actions)))\n",
    "        for i in range(len(human_actions)):\n",
    "            for j in range(len(robot_actions)):\n",
    "                uH = human_actions[i]\n",
    "                uR = robot_actions[j]\n",
    "                s_next = transition(s,uH,uR,0)\n",
    "                qH = reward(s, uH, uR)[0] + gamma*v_funs_adv[t+1][s_next]\n",
    "                A[i,j] = qH\n",
    "        rps = nash.Game(A)\n",
    "        (s1,s2) = list(rps.support_enumeration())[0]\n",
    "        v_funs_adv[t][s] = rps[s1,s2][0]\n",
    "    t -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_funs_adv[4][(\"s1\",project_risk(30))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute value function\n",
    "# t = T-1\n",
    "# while t >= 0:\n",
    "#     for s in states:\n",
    "#         qS = None\n",
    "#         uH_opt = None\n",
    "#         for uH in human_actions:\n",
    "#             for uR in robot_actions:\n",
    "#                 # Find cooperative action subject to constraints\n",
    "#                 r = reward(s, uH, uR)\n",
    "#                 qH_adv_p = r[0] + gamma*v_funs_adv_p[t+1][transition(s,uH,uR,0)]\n",
    "#                 vH_adv = v_funs_adv[t][s]\n",
    "                \n",
    "#                 # Check if action passes constraints\n",
    "#                 # print(s[1], end=\",\")\n",
    "#                 if qH_adv_p >= vH_adv - s[1]:\n",
    "#                     qS_temp = r[2] + gamma*v_funs[t+1][transition(s,uH,uR,qH_adv_p-vH_adv)] # social value of action\n",
    "#                     if qS is None or qS_temp >= qS:\n",
    "#                         qS = qS_temp\n",
    "#                         uH_opt = uH\n",
    "#         # Find value function adversarial to policy under human's reward\n",
    "#         print(uH_opt,end=\",\")\n",
    "#         v_funs_adv_p[t][s] = np.min([reward(s, uH_opt, a)[0] + gamma*v_funs_adv_p[t+1][transition(s,uH_opt,a,0)] for a in robot_actions])\n",
    "        \n",
    "#         v_funs[t][s] = qS\n",
    "#         uHs[t][s][uH_opt] = uH_opt\n",
    "#     t = t - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute value function\n",
    "# t = T-1\n",
    "# while t >= 0:\n",
    "#     for s in states:\n",
    "#         A_social = np.zeros(shape=(len(human_actions), len(robot_actions)))\n",
    "#         A_H_adv = np.zeros(shape=(len(human_actions), len(robot_actions)))\n",
    "#         for i in range(len(human_actions)):\n",
    "#             for j in range(len(robot_actions)):\n",
    "#                 uH = human_actions[i]\n",
    "#                 uR = robot_actions[j]\n",
    "\n",
    "#                 V_base = v_funs_adv[t][s]\n",
    "\n",
    "#                 dr = 0\n",
    "#                 s_next = transition(s,uH,uR,dr)\n",
    "\n",
    "#                 q_social = reward(s, uH, uR)[0] + gamma*v_funs[t+1][s_next]\n",
    "#                 q_H_adv = reward(s, uH, uR)[1] + gamma*v_funs_adv_p[t+1][s_next]\n",
    "                \n",
    "#                 A_social[i,j] = q_social\n",
    "#                 A_H_adv[i,j] = q_H_adv\n",
    "\n",
    "#         v_funs_adv_p[t][s] = 1\n",
    "        \n",
    "#         v_funs[t][s] = 1\n",
    "#         uHs[t][s][uH_opt] = 1\n",
    "#     t = t - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute value function\n",
    "# t = T-1\n",
    "# while t >= 0:\n",
    "#     for s in states:\n",
    "#         m,n = len(human_actions), len(robot_actions)\n",
    "#         A_S = np.zeros(shape=(m,n))\n",
    "#         A_H = np.zeros(shape=(m,n))\n",
    "\n",
    "#         p_joint = [[cp.Variable() for _ in range(n)] for _ in range(m)]\n",
    "#         constraints = []\n",
    "\n",
    "#         for i in range(m):\n",
    "#             for j in range(n):\n",
    "#                 uH = human_actions[i]\n",
    "#                 uR = robot_actions[j]\n",
    "\n",
    "#                 dr = v_funs_adv[t+1][transition(s,uH,uR,0)] - v_funs_adv[t][s]\n",
    "#                 s_next = transition(s,uH,uR,dr)\n",
    "#                 q_coop = reward(s, uH, uR)[0] + gamma*v_funs[t+1][s_next]\n",
    "#                 A_S[i,j] = q_coop\n",
    "\n",
    "#                 q_adv = reward(s, uH, uR)[1] + gamma*v_funs_adv_p[t+1][s_next]\n",
    "#                 A_H[i,j] = q_adv\n",
    "\n",
    "#                 constraints.append(p_joint[i][j] >= 0)\n",
    "\n",
    "#         objective = cp.Minimize(np.sum([[p_joint[i][j]*A_S[i,j] for j in range(n)] for i in range(m)]))\n",
    "\n",
    "#         constraints.append(np.sum(p_joint) == 1)\n",
    "#         for j in range(n):\n",
    "#             constraints.append(np.sum([np.sum([p_joint[i][k] for k in range(n)])*A_H[i,j] for i in range(m)]) >= v_funs_adv[t][s] - s[1])\n",
    "\n",
    "#         prob = cp.Problem(objective, constraints)\n",
    "#         v_funs[t][s] = prob.solve()\n",
    "\n",
    "#         v_funs_adv_p[t][s] = min(A_H.T @ np.array([[np.sum([p_joint[i][k].value for k in range(n)]) for i in range(m)]]).T)\n",
    "        \n",
    "#         uHs[t][s] = [[p_joint[j][i].value for i in range(n)] for j in range(m)]\n",
    "#     t = t - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute value function\n",
    "t = T-1\n",
    "while t >= 0:\n",
    "    for s in states:\n",
    "        m,n = len(human_actions), len(robot_actions)\n",
    "        A_S = np.zeros(shape=(m,n))\n",
    "        A_H = np.zeros(shape=(m,n))\n",
    "\n",
    "        p_H = cp.Variable((m,1))\n",
    "        constraints = []\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                uH = human_actions[i]\n",
    "                uR = robot_actions[j]\n",
    "\n",
    "                dr = v_funs_adv[t+1][transition(s,uH,uR,0)] - v_funs_adv[t][s]\n",
    "                s_next = transition(s,uH,uR,dr)\n",
    "                q_coop = reward(s, uH, uR)[0] + gamma*v_funs[t+1][s_next]\n",
    "                A_S[i,j] = q_coop\n",
    "\n",
    "                q_adv = reward(s, uH, uR)[1] + gamma*v_funs_adv_p[t+1][s_next]\n",
    "                A_H[i,j] = q_adv\n",
    "\n",
    "        constraints.extend([p_H>=0, cp.sum(p_H)==1])\n",
    "        for j in range(n):\n",
    "            constraints.append(p_H.T @ A_H[:,j] >= v_funs_adv[t][s] - s[1])\n",
    "\n",
    "        vals = []\n",
    "        p_Hs = []\n",
    "        for j in range(n):\n",
    "            objective = cp.Maximize(p_H.T @ A_S[:,j])\n",
    "            prob = cp.Problem(objective, constraints)\n",
    "            val_j = prob.solve()\n",
    "            vals.append(val_j)\n",
    "            p_Hs.append(p_H.value)\n",
    "\n",
    "        opt_val = np.max(vals)\n",
    "        opt_p_H = p_Hs[np.argmax(vals)]\n",
    "        v_funs[t][s] = opt_val\n",
    "        v_funs_adv_p[t][s] = np.min(opt_p_H.T @ A_H)\n",
    "        \n",
    "        uR_adv = np.zeros((n,1))\n",
    "        uR_coop = np.zeros((n,1))\n",
    "\n",
    "        uR_adv[np.argmin(opt_p_H.T @ A_H),0] = 1\n",
    "        uR_coop[np.argmax(vals),0] = 1\n",
    "\n",
    "        uRs_adv[t][s] = uR_adv\n",
    "        uRs_coop[t][s] = uR_coop\n",
    "        uHs[t][s] = opt_p_H\n",
    "    t = t - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 10]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with cooperative agent\n",
      "('s1', 1.0101010101010102)\n",
      "state= ('s1', 3.0303030303030303) , uH= 1 , dr= 2.0\n",
      "state= ('s1', 5.050505050505051) , uH= 1 , dr= 2.0\n",
      "state= ('s1', 16.161616161616163) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 27.272727272727273) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 38.38383838383839) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 49.494949494949495) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 60.60606060606061) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 71.71717171717172) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 82.82828282828284) , uH= 10 , dr= 11.0\n",
      "state= ('s1', 93.93939393939395) , uH= 10 , dr= 11.0\n",
      "\n",
      "Testing with adversarial agent\n",
      "('s1', 1.0101010101010102)\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n",
      "state= ('s1', 1.0101010101010102) , uH= 1 , dr= 0.0\n"
     ]
    }
   ],
   "source": [
    "# Simulate policy\n",
    "pol_R_adv = lambda t,s: robot_actions[np.argmax(uRs_adv[t][s])]\n",
    "pol_R_coop = lambda t,s: robot_actions[np.argmax(uRs_coop[t][s])]\n",
    "pol_H = lambda t,s:  human_actions[np.argmax(uHs[t][s])]\n",
    "\n",
    "\n",
    "print(\"Testing with cooperative agent\")\n",
    "s = (\"s1\",project_risk(1))\n",
    "print(s)\n",
    "for t in range(T):\n",
    "    uH = pol_H(t,s)\n",
    "    uR = pol_R_coop(t,s)\n",
    "    dr = reward(s, uH, uR)[0] + gamma*v_funs_adv[t+1][transition(s,uH,uR,0)]-v_funs_adv[t][(s[0],project_risk(s[1]))]\n",
    "    s = transition(s,uH,uR,dr)\n",
    "    print(\"state=\",s,\", uH=\", uH,\", dr=\", dr)\n",
    "\n",
    "print()\n",
    "print(\"Testing with adversarial agent\")\n",
    "s = (\"s1\",project_risk(1))\n",
    "print(s)\n",
    "for t in range(T):\n",
    "    uH = pol_H(t,s)\n",
    "    uR = pol_R_adv(t,s)\n",
    "    dr = reward(s, uH, uR)[0] + gamma*v_funs_adv[t+1][transition(s,uH,uR,0)]-v_funs_adv[t][(s[0],project_risk(s[1]))]\n",
    "    s = transition(s,uH,uR,dr)\n",
    "    print(\"state=\",s,\", uH=\", uH,\", dr=\", dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
